{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "Mbyy88dG-Qq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Zachary Thorman](https://github.com/zthor5)|"
      ],
      "metadata": {
        "id": "ByHvc05m-SYH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtwZ8DZGJfUv"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This Notebook will generate JSONLs & Training splits for Finetuning Embeddings from a list of PDFs using Generative AI's full context to do analysis of the PDFs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started\n",
        "\n",
        "In this section, you will install needed dependencies & define the Google Cloud project where you want to connect to Vertex AI."
      ],
      "metadata": {
        "id": "2t--gKa8HDZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "gM2it3BmSEo-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JbXe7Oodc5dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452c91f2-9bf9-435f-8613-c04cab1e682c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.1/163.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/127.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m717.3/717.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet google-generativeai chromadb pymupdf google-cloud-storage langchain==0.1.20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwmKt115PxK8"
      },
      "source": [
        "Then import the modules you'll use in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "muuhsDmmKdHi"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "import chromadb\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymupdf\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "\n",
        "from vertexai.generative_models import GenerativeModel, Part\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "from google.cloud import storage\n",
        "\n",
        "from IPython.display import Markdown, HTML, display\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "\n",
        "# Import LangChain components\n",
        "import langchain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import DataFrameLoader\n",
        "\n",
        "\n",
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6tZGHUDOCFW"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After its restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "ruVajAp-Htc4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ Wait for the kernel to finish restarting before you continue. ⚠️</b>\n",
        "</div>"
      ],
      "metadata": {
        "id": "GmqS2zdDHwki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment.\n",
        "\n",
        "This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
      ],
      "metadata": {
        "id": "ekYrnfAnH12N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "022W7-OUH5YL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Google Cloud project information, initialize Vertex AI, and add Secrets\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ],
      "metadata": {
        "id": "AALfV4LTH-xA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizing Secrets to retrieve sensitive information\n",
        "# You can add your own projectID and location to run in your environment.\n",
        "\n",
        "PROJECT_ID = userdata.get('ProjectId') # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"    # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "YgDr8fuQIAbQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions for Creating Fine Tuning Data"
      ],
      "metadata": {
        "id": "ccPd69qZJN2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating JSONLs\n",
        "\n",
        "Note: *Markdown is currently lost in this conversion.*"
      ],
      "metadata": {
        "id": "ILO1NZw--xnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pauses execution on GCP for 12 second due to default Quota for Vertex AI. Determines amount of reattempts\n",
        "defualt_quota_sec = 12\n",
        "default_attempts = 5\n",
        "\n",
        "# Defines Training Splits for Training, Validation, & Testing.tsv\n",
        "TRAINING_SPLIT = 0.8\n",
        "VALIDATION_SPLIT = 0.1\n",
        "TESTING_SPLIT = 0.1\n",
        "\n",
        "# Create a text splitter to divide documents into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=10000,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "\n",
        "# Creating the Generation Config\n",
        "generation_config = {\n",
        "\"max_output_tokens\": 8192,\n",
        "\"temperature\": 0,\n",
        "\"top_p\": 0.95,\n",
        "}\n",
        "\n",
        "# Defining Safety filters that WILL NOT block (hopefully) the content outputted\n",
        "safety_settings = {\n",
        "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
        "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
        "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
        "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
        "}\n"
      ],
      "metadata": {
        "id": "weOCHmSHQZL1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass The folder path for storing the images\n",
        "def create_clean_folders(PDF_Path):\n",
        "  # Create the directory if it doesn't exist\n",
        "  if not os.path.exists(PDF_Path):\n",
        "    os.makedirs(PDF_Path)\n",
        "  pdf_star = PDF_Path + \"*\"\n",
        "  !rm -rf {pdf_star}\n",
        "\n",
        "  if not os.path.exists(\"./output/\"):\n",
        "    os.makedirs(\"./output/\")\n",
        "  pdf_star = \"./output/\" + \"*\"\n",
        "  !rm -rf {pdf_star}\n",
        "\n",
        "def update_text(text = \"default text\"):\n",
        "    return HTML(\"\"\"\n",
        "        <p>{}</p>\n",
        "    \"\"\".format(text))\n",
        "\n",
        "def progress(value =1, max =1):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 60%'>\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "\n",
        "def download_bucket_to_local(bucket_uri, local_folder):\n",
        "  gcs_uri_list = []\n",
        "  storage_client = storage.Client()\n",
        "  bucket = storage_client.bucket(bucket_uri)\n",
        "  blobs = bucket.list_blobs()\n",
        "  for blob in blobs:\n",
        "    file_path = local_folder + blob.name\n",
        "    blob.download_to_filename(file_path)\n",
        "    gcs_uri_list.append(\"gs://\" + bucket_uri + \"/\" + blob.name)\n",
        "    print(f\"Downloaded: {blob.name}\")\n",
        "  return gcs_uri_list\n",
        "\n",
        "\n",
        "def pdf_context_jsonl(pdf_folder, model):\n",
        "  all_pdfs_text = \"\"\n",
        "  for pdf in os.listdir(pdf_folder):\n",
        "    with pymupdf.open(pdf_folder + pdf) as doc:\n",
        "      for page in doc:\n",
        "          all_pdfs_text += page.get_text()\n",
        "\n",
        "  # Split the text into chunks\n",
        "  chunks = text_splitter.split_text(all_pdfs_text)\n",
        "  print(f'Now proccessing ({len(chunks)}) chunks through LLM...')\n",
        "  with open('./output/context.jsonl', 'w') as f:\n",
        "    display_out = display(progress(0, len(chunks)), display_id=True)\n",
        "    for i, chunk in enumerate(chunks):\n",
        "      chunk = chunk.replace(\"\\n\",\"\")\n",
        "      chunk = chunk.replace('\"','\\\\\\\"')\n",
        "      is_quota_too_low = 0\n",
        "      current_attmepts = 0\n",
        "      while current_attmepts < default_attempts:\n",
        "        try:\n",
        "          title = model.generate_content(f\"Generate a 10 word summary of this text: {chunk}\")\n",
        "          current_attmepts = default_attempts + 1\n",
        "        except Exception as err:\n",
        "          if is_quota_too_low > default_attempts * 5:\n",
        "            raise Exception(\"Cancelling Process. Please either:\\n - Increase [defualt_quota_sec] to allow for longer pauses between API calls\\n - Request a Quota Increase request for your API\")\n",
        "          print(f\"LLM's need breaks too! Reattempting {current_attmepts+1}/{default_attempts}. Pausing for {defualt_quota_sec} seconds.\")\n",
        "          current_attmepts += 1\n",
        "          is_quota_too_low += 1\n",
        "          time.sleep(defualt_quota_sec)\n",
        "\n",
        "      display_out.update(progress(i, len(chunks)))\n",
        "      cleansed_title = title.text.replace(\"\\n\",\"\").replace('\"','\\\\\"')\n",
        "      f.write(f'{{\"_id\":\"context_{i}\",\"title\":\"{cleansed_title}\",\"text\":\"{chunk}\"}}\\n')\n",
        "\n",
        "    display_out.update(progress())\n",
        "    print(f'Finished generating Context.jsonl')\n",
        "    f.close()\n",
        "  return all_pdfs_text\n",
        "\n",
        "# *zthor* Later validate via JSONLines\n",
        "def validate_jsonl(jsonl_text, model):\n",
        "  for line in jsonl_text.splitlines():\n",
        "    try:\n",
        "      json_line = json.loads(line)\n",
        "    except Exception as err:\n",
        "      print(f\"Invalid JSON Line: {line}\")\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "\n",
        "# Creates JSONL Prompts for a PDF and writes them into a file\n",
        "# *zthor* Modify Prompt to generate reliably at least 10 to 50 Per PDF\n",
        "def pdf_query_jsonl(gcs_pdfList, model):\n",
        "  print('Creating prompt.jsonl [May take a few minutes..]')\n",
        "  validate_text = \"\"\n",
        "  with open('./output/query.jsonl', 'w') as f:\n",
        "    display_out = display(progress(0, len(gcs_pdfList)), display_id=True)\n",
        "    for x, pdf in enumerate(gcs_pdfList):\n",
        "      pdf_file = Part.from_uri(pdf, mime_type=\"application/pdf\")\n",
        "      prompt = f'Output up to 20 questions that can be answered based on the content of the pdf provided. Output each question on a new line.'\n",
        "      output = model.generate_content([prompt,pdf_file])\n",
        "      for i, line in enumerate(output.text.splitlines()):\n",
        "        clean_output = line.replace(\"\\n\",\"\").replace('\"','\\\\\"')\n",
        "        if x+1 == len(gcs_pdfList) and i+1 == len(output.text.splitlines()):\n",
        "          clean_json = f'{{\"_id\":\"query_{x}_{i}\",\"text\":\"{clean_output}\"}}'\n",
        "          f.write(clean_json)\n",
        "          validate_text += clean_json\n",
        "        else:\n",
        "          clean_json = f'{{\"_id\":\"query_{x}_{i}\",\"text\":\"{clean_output}\"}}\\n'\n",
        "          f.write(clean_json)\n",
        "          validate_text += clean_json\n",
        "        display_out.update(progress(x, len(gcs_pdfList)))\n",
        "\n",
        "  is_validated = validate_jsonl(validate_text,model)\n",
        "  if (is_validated):\n",
        "    display_out.update(progress())\n",
        "    print(f\"Validate_jsonl_llm returned: {is_validated}\")\n",
        "    f.close()\n",
        "    return \"Successful creation of prompt.jsonl\"\n",
        "  else:\n",
        "    if 'yes' in input('Failed creation of prompt.jsonl; Reattempt creation? (yes or no): ').lower():\n",
        "      f.close()\n",
        "      pdf_query_jsonl(gcs_pdfList, model)\n",
        "    else:\n",
        "      f.close()\n",
        "      display_out.update(progress())\n",
        "      return \"Potential failed creation of prompt.jsonl\"\n",
        "\n",
        "\n",
        "def create_pairing_file(pair_list, num_of_pairs, tsv_file_loc):\n",
        "  with open(tsv_file_loc, 'w') as f:\n",
        "    f.write(\"query-id\\tcorpus-id\\tscore\\n\")\n",
        "    for i in range(num_of_pairs):\n",
        "      random_index = random.randint(0, len(pair_list)-1)\n",
        "      random_element = pair_list.pop(random_index)\n",
        "      f.write(f\"{random_element[0]}\\t{random_element[1]}\\t{random_element[2]}\")\n",
        "      if i != len(pair_list)-1:\n",
        "        f.write(\"\\n\")\n",
        "    f.close()\n",
        "  return pair_list\n",
        "\n",
        "\n",
        "# Requires JSONLs to already have been created\n",
        "def create_pairing_tsv(pdf_text, model):\n",
        "  z_progress = 0\n",
        "  total_lines = sum(1 for _ in open('./output/query.jsonl')) * sum(1 for _ in open('./output/context.jsonl'))\n",
        "  print('Creating pairing.tsv via [DEEP ANALYSIS] [This will take a couple hours..]') # 5 PDFs / 100 Prompts => 3.5 Hours\n",
        "  display_out = display(progress(0, 1), display_id=True)\n",
        "  display_out_text = display(update_text(\"Currently running...\"), display_id=True)\n",
        "  all_pairs = [[\"query-id\",\"corpus-id\",\"score\"]]\n",
        "  with open('./output/query.jsonl', 'r') as f:\n",
        "   for query_line in f:\n",
        "    try:\n",
        "      query_json_line = json.loads(query_line)\n",
        "    except Exception as err:\n",
        "      raise Exception(f\"Some invalid JSON slipped into the query.jsonl!\\nInvalid JSON [{query_line}]\\n Here is the error: {err}\")\n",
        "    with open('./output/context.jsonl', 'r') as g:\n",
        "      for context_line in g:\n",
        "        z_progress += 1\n",
        "        try:\n",
        "          context_json_line = json.loads(context_line)\n",
        "        except Exception as err:\n",
        "          raise Exception(f\"Some invalid JSON slipped into the context.jsonl!\\nInvalid JSON [{context_line}]\\n Here is the error: {err}\")\n",
        "        query = query_json_line[\"text\"]\n",
        "        content = context_json_line[\"text\"]\n",
        "        prompt = f\"Respond with only an integer that describes how well the context answers the question. The integer can be from 1 (Does not contatin any relevant information to answer the question) to 10 (Directly has information to answer the question).\\nThe question: {query}\\nThe context: {content}\"\n",
        "        is_quota_too_low = 0\n",
        "        current_attmepts = 0\n",
        "        while current_attmepts < default_attempts:\n",
        "          try:\n",
        "            response = model.generate_content(prompt)\n",
        "            current_attmepts = default_attempts + 1\n",
        "            is_quota_too_low = 0\n",
        "          except Exception as err:\n",
        "            if is_quota_too_low > default_attempts * 3:\n",
        "              raise Exception(\"Cancelling Process. Please either:\\n - Increase [defualt_quota_sec] to allow for longer pauses between API calls\\n - Request a Quota Increase for the API\")\n",
        "            display_out_text.update(update_text(f\"#{z_progress} | LLM's need breaks too! Reattempting {current_attmepts+1}/{default_attempts}. Pausing for {defualt_quota_sec} seconds.\"))\n",
        "            #print(f\"#{z_progress} | LLM's need breaks too! Reattempting {current_attmepts+1}/{default_attempts}. Pausing for {defualt_quota_sec} seconds.\")\n",
        "            current_attmepts += 1\n",
        "            is_quota_too_low += 1\n",
        "            time.sleep(defualt_quota_sec)\n",
        "\n",
        "\n",
        "        response_only_int = int(re.search(r'\\d+', response.text).group())\n",
        "        if response_only_int != 1:\n",
        "          pairing = [query_json_line[\"_id\"], context_json_line[\"_id\"], response_only_int]\n",
        "          all_pairs.append(pairing)\n",
        "        display_out.update(progress(z_progress, total_lines))\n",
        "        # This Line is only for testing\n",
        "        # if (z_progress > 200):\n",
        "        #   g.close()\n",
        "        #   break\n",
        "        # This line is only for Testing\n",
        "    g.close()\n",
        "  f.close()\n",
        "\n",
        "  display_out.update(progress(1,1))\n",
        "  print(\"Completed Deep Analysis of query <-> Context!\")\n",
        "  time.sleep(3)\n",
        "  print(f\"Length of all_pairs: {len(all_pairs)}\")\n",
        "  total_pairs = len(all_pairs)\n",
        "  if (TRAINING_SPLIT + VALIDATION_SPLIT + TESTING_SPLIT != 1):\n",
        "    raise Exception(\"TRAINING_SPLIT + VALIDATION_SPLIT + TESTING_SPLIT must equal 1\")\n",
        "  all_pairs = create_pairing_file(all_pairs, int(TRAINING_SPLIT * total_pairs), './output/training.tsv')\n",
        "  all_pairs = create_pairing_file(all_pairs, int(VALIDATION_SPLIT * total_pairs), './output/validation.tsv')\n",
        "  all_pairs = create_pairing_file(all_pairs, len(all_pairs), './output/testing.tsv')\n",
        "\n",
        "\n",
        "\n",
        "def upload_output_to_bucket():\n",
        "  gcs_bucket = f\"gs://{PROJECT_ID}-output\"\n",
        "  client = storage.Client()\n",
        "  try:\n",
        "    client.get_bucket(gcs_bucket.split(\"/\")[-1]).exists()\n",
        "  except Exception as err:\n",
        "    !gcloud storage buckets create {gcs_bucket} --location={LOCATION} --project={PROJECT_ID} --no-user-output-enabled\n",
        "  uid = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "  gcs_output = f\"/output-{uid}/\"\n",
        "  local_uri = f\"./output/.\"\n",
        "  !gsutil -q -m cp -r {local_uri} {gcs_bucket + gcs_output}\n",
        "  print(f\"Uploaded Output to GCS bucket: {gcs_bucket + gcs_output}\")"
      ],
      "metadata": {
        "id": "fGl4UCyF-27k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For More details on what is needed for Fine Tuning, [learn more here!](https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings#dataset-format)"
      ],
      "metadata": {
        "id": "IftfNL07wR7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\"gemini-1.5-pro-preview-0514\", generation_config = generation_config, safety_settings=safety_settings)\n",
        "\n",
        "# Only Store PDFs in the Bucket\n",
        "gcs_bucket = \"dmv-pdf-analysis\" # Do not put any slashes after uri!\n",
        "pdf_folder =\"./downloaded_pdfs/\" # Include a slash after the uri!\n",
        "\n",
        "create_clean_folders(pdf_folder)\n",
        "gcs_pdf_list = download_bucket_to_local(gcs_bucket, pdf_folder)\n",
        "\n",
        "all_pdf_text = pdf_context_jsonl(pdf_folder, model)\n",
        "pdf_query_jsonl(gcs_pdf_list, model)\n",
        "\n",
        "timer = time.perf_counter()\n",
        "create_pairing_tsv(all_pdf_text, model)\n",
        "timer = time.perf_counter() - timer\n",
        "print(f'Finished (DEEP ANALYSIS) of create_pairing_tsv in {timer} seconds.')\n",
        "upload_output_to_bucket()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "Bubx2SO2IsiK",
        "outputId": "f3db631e-12e2-4b44-aa47-df462b39f203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: 40_hour_teen_driving_guide.pdf\n",
            "Downloaded: alcohol_drug_awareness_student.pdf\n",
            "Downloaded: commercial_driver_guide.pdf\n",
            "Downloaded: driver_manual_ga_2024.pdf\n",
            "Downloaded: motorcycle_operator_guide.pdf\n",
            "Now proccessing (109) chunks through LLM...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='1'\n",
              "            max='1',\n",
              "            style='width: 60%'>\n",
              "            1\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished generating Context.jsonl\n",
            "Creating prompt.jsonl [May take a few minutes..]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='1'\n",
              "            max='1',\n",
              "            style='width: 60%'>\n",
              "            1\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validate_jsonl_llm returned: True\n",
            "Creating pairing.tsv via [DEEP ANALYSIS] [This will take a couple hours..]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='2284'\n",
              "            max='14606',\n",
              "            style='width: 60%'>\n",
              "            2284\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <p>#2233 | LLM's need breaks too! Reattempting 2/5. Pausing for 12 seconds.</p>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThTbjAJ7eGP5"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "To learn more about how you can use the embeddings, check out the [examples](https://ai.google.dev/examples?keywords=embed) available. To learn how to use other services in the Gemini API, visit the [Python quickstart](https://ai.google.dev/gemini-api/docs/get-started/python)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2t--gKa8HDZ7",
        "U6tZGHUDOCFW",
        "ekYrnfAnH12N"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}